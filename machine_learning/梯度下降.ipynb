{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降是一种优化算法，主要用于最小化目标函数，如损失函数，在机器学习和深度学习中用于训练模型。其核心原理是沿着目标函数梯度的反方向逐步调整参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x,y,lr = 0.01,n_iters = 1000):\n",
    "    m,n = x.shape\n",
    "    w = np.zeros(n)  \n",
    "\n",
    "    for _ in range(n_iters):\n",
    "        # 计算预测值\n",
    "        y_pred = np.dot(x,w)\n",
    "        # 计算梯度\n",
    "        gradient =  np.dot(x.T, (y_pred - y)) / m \n",
    "        # 更新参数\n",
    "        w = w - lr * gradient\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最优参数： [0.10065471 0.42053884]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2],[3,4],[5,6]])  # (3,2)\n",
    "y = np.array([1,2,3])    # (3,)  \n",
    "w = gradient_descent(x,y)   \n",
    "w  # (2,)\n",
    "print(\"最优参数：\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  计算梯度\n",
    "NumPy本身不直接提供自动求导功能，但可以通过数值方法（如有限差分法）来近似计算梯度\n",
    "\n",
    "torch 提供了自动求导功能， 定义tensor时， set requires_grad = True \n",
    "\n",
    "对函数求梯度 $$ y=x^2+3x$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 16.   13.   31.    9.   13. ]\n",
      " [ -7.    2.   10.5  10.5   9. ]\n",
      " [-30.   -9.  -10.   12.    5. ]]\n",
      "[[ -8.   -8.5   1.    0.  -11. ]\n",
      " [-11.   -1.   -1.   -9.   -7. ]\n",
      " [ 10.    9.    9.5  -1.5 -14. ]]\n"
     ]
    }
   ],
   "source": [
    "c = np.random.randint(0, 50, (3, 5))  # 二维数组示例\n",
    "grad_c = np.gradient(c)\n",
    "for grad in grad_c:\n",
    "    print(grad)  # 分别打印每个维度的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1.0,2.0], requires_grad=True)\n",
    "\n",
    "def fun(x):\n",
    "    return x[0]**2 + 3*x[1]\n",
    "\n",
    "y = fun(x)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "gradient = x.grad\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: x = 8.0, f(x) = 88.0\n",
      "Iteration 10: x = 0.8589934592000003, f(x) = 3.3148501405483835\n",
      "Iteration 20: x = 0.09223372036854777, f(x) = 0.28520822027866677\n",
      "Iteration 30: x = 0.009903520314283045, f(x) = 0.029808640657464552\n",
      "Iteration 40: x = 0.001063382396627933, f(x) = 0.0031912779720052573\n",
      "Iteration 50: x = 0.00011417981541647683, f(x) = 0.00034255248327967906\n",
      "Iteration 60: x = 1.2259964326927117e-05, f(x) = 3.678004328750665e-05\n",
      "Iteration 70: x = 1.3164036458569655e-06, f(x) = 3.949212670489455e-06\n",
      "Iteration 80: x = 1.4134776518227082e-07, f(x) = 4.2404331552600316e-07\n",
      "Iteration 90: x = 1.5177100720513518e-08, f(x) = 4.553130239188494e-08\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义函数和梯度\n",
    "def function(x):\n",
    "    return x**2 + 3*x\n",
    "\n",
    "def gradient(x):\n",
    "    return 2 * x\n",
    "\n",
    "# 梯度下降算法\n",
    "def gradient_descent(initial_x, learning_rate, num_iterations):\n",
    "    x = initial_x\n",
    "    for i in range(num_iterations):\n",
    "        grad = gradient(x)  # 计算梯度\n",
    "        x = x - learning_rate * grad  # 更新参数\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Iteration {i}: x = {x}, f(x) = {function(x)}\")\n",
    "    return x\n",
    "\n",
    "# 初始参数\n",
    "initial_x = 10.0\n",
    "learning_rate = 0.1\n",
    "num_iterations = 100\n",
    "optimal_x = gradient_descent(initial_x, learning_rate, num_iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
