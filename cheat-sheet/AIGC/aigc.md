https://github.com/QwenLM/Qwen/issues/604

model.chat and model.genarate





### vllm

llm  的推理部署库， 可以提升服务的性能

算法： page attention

将连续的kv cache 存储在不连续的空间， 避免显存的浪费





llama index